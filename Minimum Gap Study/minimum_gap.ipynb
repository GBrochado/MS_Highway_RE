{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Study on the Minimum Gap between Highway Cars for the Ego to Merge into the Highway**\n",
    "\n",
    "##### This project aims to determine the minimum gap that two highway vehicles should maintain to allow a merging car (ego vehicle) to safely merge between them. The process begins by evaluating a range of gap intervals to assess which performs best in terms of safety and traffic flow. This is done by adjusting the 'minimum_gap_range' in the environment's configuration. Subsequently, the range of this gap interval will be gradually narrowed down to identify the exact gap(s) that offer the safest and most efficient merging conditions, with the goal of pinpointing a single optimal gap or multiple gaps if no clear standout emerges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint\n",
    "from IPython.display import Video\n",
    "import cv2\n",
    "import imageio\n",
    "import highway_env\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from highway_env import utils\n",
    "from highway_env.envs import MergeEnv\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creation of the environment**\n",
    "\n",
    "##### With the ego-vehicle on the merging lane and two vehicles on the highway, on the right most lane and with a certain distance between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightLaneVehicle(ControlledVehicle):\n",
    "    \"\"\"\n",
    "    Um veículo que é restrito a ficar na lane da direita e nunca muda de lane.\n",
    "    \"\"\"\n",
    "    def act(self, action: int = None) -> None:\n",
    "        # Assegura que o veículo não mude de lane (desautoriza ações 0 e 2 para mudança de lane)\n",
    "        if action in [0, 2]:  # Ações para mudar para a esquerda ou direita\n",
    "            action = 1  # Forçar a manter a lane (ação 1)\n",
    "        super().act(action)\n",
    "\n",
    "\n",
    "class CustomMergeEnv(MergeEnv):\n",
    "    def __init__(self, distance, *args, **kwargs):\n",
    "        # Set the distance attribute first\n",
    "        self.distance = distance  # Set the distance before calling the base class\n",
    "        # Initialize the base class (MergeEnv) after setting the distance\n",
    "        super().__init__(*args, **kwargs)  # This ensures the parent class is properly initialized\n",
    "\n",
    "    def _make_vehicles(self) -> None:\n",
    "        # Ensure that distance is properly initialized before use\n",
    "        road = self.road\n",
    "\n",
    "        # Ponto de mesclagem (merge) na lane 0\n",
    "        merge_position = road.network.get_lane((\"b\", \"c\", 0)).position(0, 0)  # Ponto de mesclagem na autoestrada\n",
    "        \n",
    "        # Posição inicial do veículo ego na lane de mesclagem\n",
    "        ego_initial_position = road.network.get_lane((\"j\", \"k\", 0)).position(30, 0)  # Ego vehicle na lane de mesclagem\n",
    "\n",
    "        # Ajustar a posição inicial do veículo da autoestrada com base na distância fornecida\n",
    "        highway_vehicle_initial_position = road.network.get_lane((\"a\", \"b\", 1)).position(100 - self.distance, 0)  # Ajuste na posição de acordo com a distância\n",
    "        highway_vehicle_initial_position_1 = road.network.get_lane((\"a\", \"b\", 1)).position(100 - self.distance, 0)  # Outro veículo ajustado pela mesma distância\n",
    "\n",
    "        # Definir velocidades iniciais\n",
    "        ego_speed = 20  # Velocidade inicial do ego\n",
    "\n",
    "        # Calcular o tempo para ambos os veículos chegarem ao ponto de mesclagem\n",
    "        time_to_merge = (merge_position[0] - ego_initial_position[0]) / ego_speed\n",
    "\n",
    "        # Ajustar a velocidade do veículo da autoestrada para garantir que ambos cheguem ao mesmo tempo\n",
    "        highway_vehicle_speed = (merge_position[0] - highway_vehicle_initial_position[0]) / time_to_merge\n",
    "\n",
    "        # Criar o veículo ego na lane de mesclagem\n",
    "        ego_vehicle = self.action_type.vehicle_class(\n",
    "            road, ego_initial_position, speed=ego_speed\n",
    "        )\n",
    "        road.vehicles.append(ego_vehicle)\n",
    "\n",
    "        # Criar o veículo na lane da direita da autoestrada (lane 1)\n",
    "        highway_vehicle = RightLaneVehicle(\n",
    "            road, highway_vehicle_initial_position, speed=highway_vehicle_speed\n",
    "        )\n",
    "        road.vehicles.append(highway_vehicle)\n",
    "\n",
    "        # Criar o segundo veículo na mesma lane\n",
    "        highway_vehicle_1 = RightLaneVehicle(\n",
    "            road, highway_vehicle_initial_position_1, speed=highway_vehicle_speed\n",
    "        )\n",
    "        road.vehicles.append(highway_vehicle_1)\n",
    "\n",
    "        # Definir o veículo ego como o veículo principal\n",
    "        self.vehicle = ego_vehicle\n",
    "\n",
    "    def set_distance(self, distance: int) -> None:\n",
    "        \"\"\"\n",
    "        Método para alterar a distância entre os veículos no ambiente.\n",
    "        \"\"\"\n",
    "        self.distance = distance  # Atualiza a distância\n",
    "        self._make_vehicles()  # Recria os veículos com a nova distância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the custom environment\n",
    "gym.envs.registration.register(\n",
    "    id='CustomMerge-v0',\n",
    "    entry_point='__main__:CustomMergeEnv', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **With distance 20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_20 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array', distance=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **With distance 40**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_40 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array', distance=40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **With distance 60**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_60 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array', distance=60) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **With distance 80**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_80 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array', distance=80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **With distance 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_100 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array', distance=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_20,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_minimum_gap_20/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_minimum_gap_20/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_40,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_minimum_gap_40/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_minimum_gap_40/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_60,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_minimum_gap_60/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_minimum_gap_60/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_80,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_minimum_gap_80/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_minimum_gap_80/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_100,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_minimum_gap_100/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_minimum_gap_100/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluate and compare the models**\n",
    "\n",
    "**For the minimum gap 20**\n",
    "- Average Reward:\n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For the minimum gap 40**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For the minimum gap 60**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions:\n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For the minimum gap 80**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For the minimum gap 100**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(model, env, num_episodes, speed_threshold_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Função para avaliar um modelo em um ambiente específico.\n",
    "\n",
    "    Parâmetros:\n",
    "    - model: o modelo de aprendizado a ser avaliado.\n",
    "    - env: o ambiente no qual o modelo será avaliado.\n",
    "    - num_episodes: número de episódios para a avaliação.\n",
    "    - speed_threshold_ratio: fator que determina o threshold de velocidade (default 0.3).\n",
    "\n",
    "    Retorno:\n",
    "    - Um dicionário com as métricas de avaliação: recompensa média, número de colisões, número de fusões bem-sucedidas, etc.\n",
    "    \"\"\"\n",
    "    total_rewards = []  # Lista para armazenar as recompensas totais por episódio\n",
    "    total_collisions = 0  # Contador para colisões\n",
    "    successful_merges = 0  # Contador para fusões bem-sucedidas\n",
    "    dangerous_driving_episodes = 0  # Contador para episódios com direção perigosa\n",
    "    total_steps_to_merge = []  # Lista para armazenar os passos até a fusão\n",
    "    total_episode_times = []  # Lista para armazenar o tempo de cada episódio\n",
    "\n",
    "    # Cálculo do threshold de velocidade\n",
    "    reward_speed_range = env.config[\"reward_speed_range\"]\n",
    "    speed_threshold = (reward_speed_range[1] - reward_speed_range[0]) * speed_threshold_ratio  # Limite para mudanças repentinas de velocidade\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        start_time = time.time()  # Registrar o tempo de início do episódio\n",
    "        obs, info = env.reset()  # Resetar o ambiente e pegar a observação inicial\n",
    "        done = False  # Variável para verificar se o episódio terminou\n",
    "        episode_reward = 0  # Variável para acumular a recompensa do episódio\n",
    "        collisions = 0  # Contador de colisões neste episódio\n",
    "        dangerous_driving = False  # Flag para direção perigosa\n",
    "        steps_to_merge = 0  # Contador de passos até a fusão\n",
    "        last_speed = None  # Inicializar a velocidade anterior como None\n",
    "\n",
    "        # Armazenar as posições dos veículos na rodovia para verificar a fusão\n",
    "        highway_vehicle_positions = []\n",
    "\n",
    "        while not done:  # Loop até o episódio terminar\n",
    "            # O agente escolhe uma ação\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            # Executar a ação no ambiente\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Armazenar as posições dos veículos na rodovia\n",
    "            if 'highway_vehicles' in info:\n",
    "                highway_vehicle_positions = [vehicle.position[0] for vehicle in info['highway_vehicles']]\n",
    "\n",
    "            dangerous_driving = False  # Resetar flag de direção perigosa\n",
    "            episode_reward += reward  # Acumular recompensa do episódio\n",
    "            steps_to_merge += 1  # Incrementar os passos até a fusão\n",
    "\n",
    "            # Verificar a velocidade atual e arredondar para 2 casas decimais\n",
    "            current_speed = round(info.get('speed', 0), 2)\n",
    "\n",
    "            # Verificar mudanças repentinas de velocidade\n",
    "            if last_speed is not None and abs(current_speed - last_speed) > speed_threshold:\n",
    "                dangerous_driving = True  # Marcar como direção perigosa se a mudança de velocidade for acima do threshold\n",
    "\n",
    "            last_speed = current_speed  # Atualizar a velocidade anterior para a próxima iteração\n",
    "\n",
    "            # Verificar colisões\n",
    "            if 'crashed' in info and info['crashed']:\n",
    "                collisions += 1  # Incrementar o contador de colisões\n",
    "\n",
    "            # Verificar se o episódio terminou (terminado ou truncado)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Considerar uma fusão bem-sucedida se o veículo ego estiver entre os dois veículos da rodovia\n",
    "        if highway_vehicle_positions:\n",
    "            ego_vehicle_position = info['ego_vehicle_position'][0]  # Coordenada x do veículo ego\n",
    "            highway_vehicle_position_1 = highway_vehicle_positions[0]\n",
    "            highway_vehicle_position_2 = highway_vehicle_positions[1]\n",
    "\n",
    "            # Verificar se o veículo ego está entre os dois veículos da rodovia\n",
    "            if (highway_vehicle_position_1 < ego_vehicle_position < highway_vehicle_position_2) or \\\n",
    "               (highway_vehicle_position_2 < ego_vehicle_position < highway_vehicle_position_1):\n",
    "                successful_merges += 1  # Incrementar fusões bem-sucedidas se o veículo ego estiver entre os veículos da rodovia\n",
    "\n",
    "        # Logar as métricas do episódio\n",
    "        total_rewards.append(episode_reward)  # Adicionar recompensa do episódio à lista\n",
    "        total_collisions += collisions  # Atualizar o total de colisões\n",
    "        total_steps_to_merge.append(steps_to_merge)  # Adicionar passos para fusão\n",
    "\n",
    "        if dangerous_driving:\n",
    "            dangerous_driving_episodes += 1  # Incrementar contagem de episódios com direção perigosa\n",
    "\n",
    "        # Calcular o tempo do episódio\n",
    "        episode_time = time.time() - start_time  # Calcular o tempo total do episódio\n",
    "        total_episode_times.append(episode_time)  # Adicionar o tempo do episódio à lista\n",
    "\n",
    "    # Calcular métricas finais\n",
    "    avg_reward = np.mean(total_rewards)  # Recompensa média\n",
    "    avg_steps_to_merge = np.mean(total_steps_to_merge)  # Passos médios até a fusão\n",
    "    avg_episode_time = np.mean(total_episode_times)  # Tempo médio de episódio\n",
    "\n",
    "    # Exibir resultados\n",
    "    print(f\"Average Reward: {avg_reward}\")  # Exibir recompensa média\n",
    "    print(f\"Average Steps to Merge: {avg_steps_to_merge}\")  # Exibir passos médios até a fusão\n",
    "    print(f\"Average Episode Time: {avg_episode_time:.2f} seconds\")  # Exibir tempo médio de episódio\n",
    "    print(f\"Number of Collisions: {total_collisions}\")  # Exibir número de colisões\n",
    "    print(f\"Successful Merges: {successful_merges}\")  # Exibir número de fusões bem-sucedidas\n",
    "    print(f\"Number of Dangerous Driving Episodes (sudden speed changes): {dangerous_driving_episodes}\")  # Exibir episódios com direção perigosa\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": avg_reward,  # Recompensa média\n",
    "        \"avg_steps_to_merge\": avg_steps_to_merge,  # Passos médios até a fusão\n",
    "        \"avg_episode_time\": avg_episode_time,  # Tempo médio de episódio\n",
    "        \"number_collisions\": total_collisions,  # Número total de colisões\n",
    "        \"successful_merges\": successful_merges,  # Número de fusões bem-sucedidas\n",
    "        \"number_dangerous_episodes\": dangerous_driving_episodes  # Número de episódios com direção perigosa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_minimum_gap_20/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_20, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_minimum_gap_40/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_40, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_minimum_gap_60/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_60, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_minimum_gap_80/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_80, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_minimum_gap_100/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_100, 200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textinho a analisar os resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Narrowing the XXXXXXXX distance interval**\n",
    "\n",
    "MUDAR ESTE TEXTO PARA FALAR ACERCA DO INTERVALO ÓTIMO E DIZER QUE É DE 2 EM 2\n",
    "\n",
    "##### In order to identify the optimal speed range within the broader interval of (10, 20), a process of gradual refinement was employed. By breaking this interval into smaller subintervals of 0.5, such as (10, 10.5), (10.5, 11), and so on, each subinterval is evaluated independently using the same metrics as before. The goal of this narrowing process is to identify which specific speed range yields the highest rewards, minimizes collisions, and reduces dangerous driving episodes. By successively refining these subintervals and analyzing the results, we can pinpoint the exact optimal speed range where the agent performs most efficiently and safely. This step-by-step method ensures that performance is maximized within the interval of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distance_intervals(model, env, base_interval, step_size, num_episodes):\n",
    "    \"\"\"\n",
    "    Função para avaliar o agente em diferentes intervalos de distância entre os veículos.\n",
    "    \n",
    "    model: modelo do agente (PPO, etc.)\n",
    "    env: o ambiente onde a simulação ocorre\n",
    "    base_interval: intervalo inicial de distância (ex. 40)\n",
    "    step_size: decremento para cada subintervalo de distância (ex. 2 metros)\n",
    "    num_episodes: número de episódios de simulação para cada subintervalo\n",
    "    \n",
    "    Returns: um dicionário com os resultados para cada subintervalo.\n",
    "    \"\"\"\n",
    "    # Criação dos subintervalos de distância (decrementando de 'step_size' em 'step_size')\n",
    "    subintervals = list(range(base_interval, 0, -step_size))  # Lista de distâncias, de base_interval até 0 ou o mínimo desejado\n",
    "    \n",
    "    results = {}  # Dicionário para armazenar os resultados\n",
    "\n",
    "    for distance in subintervals:\n",
    "        print(f\"Evaluando para a distância de {distance} metros\")  # Imprimir a distância que está sendo avaliada\n",
    "        \n",
    "        # Ajustar o ambiente com a nova distância\n",
    "        env = gym.make(\"CustomMerge-v0\", render_mode='rgb_array', distance=distance)  # Criar o ambiente com a distância ajustada\n",
    "        \n",
    "        result = evaluate_agent(model, env, num_episodes)  # Avaliar o agente para essa distância\n",
    "        results[distance] = result  # Armazenar o resultado para essa distância\n",
    "        \n",
    "        print(\"\\n\")  # Espaçamento entre os resultados\n",
    "\n",
    "    return results  # Retornar os resultados para todos os intervalos de distância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros para o intervalo base e tamanho do subintervalo\n",
    "base_interval = 40  # Distância inicial de 40 metros   MUDAR AQUI\n",
    "step_size = 2  # O tamanho de cada subintervalo (reduzir a distância em 2 metros)\n",
    "num_episodes = 200  # Número de episódios para avaliar\n",
    "\n",
    "# Carregar o modelo\n",
    "model = PPO.load(\"modelo_40\", custom_objects={\"observation_space\": env_40.observation_space, \"action_space\": env_40.action_space}) MUDAR AQUI\n",
    "\n",
    "# Avaliar o modelo nos subintervalos de distância\n",
    "results = evaluate_distance_intervals(model, env_40, base_interval, step_size, num_episodes)  # Chama a função para avaliar os modelos MUDAR AQUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results of the evaluations\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold the data\n",
    "data = [] \n",
    "\n",
    "# Iterate through the results and flatten the structure\n",
    "for speed_range, metrics in results.items():  # Loop through each speed range and its corresponding metrics\n",
    "    row = {'Speed Range': f\"{speed_range[0]} - {speed_range[1]}\"}  # Create a new dictionary for the current row with the speed range\n",
    "    row.update(metrics)  # Add metrics to the row dictionary\n",
    "    data.append(row)  # Append the row to the data list\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "results_df = pd.DataFrame(data)  # Convert the list of dictionaries into a Pandas DataFrame\n",
    "\n",
    "results_df  # Display the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXTINHO A ANALISAR OS RESULTADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUDAR O CÓDIGO PARA GERAR VIDEO DO MODELO MELHOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"vel_study_10_20/model\")\n",
    "\n",
    "# Initialize the environment and variables for recording\n",
    "frames = []\n",
    "obs, info = env_10_20.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "max_steps = 1000\n",
    "\n",
    "# Resize frame to be divisible by 16 (macro block size for video codecs)\n",
    "def resize_frame_to_macro_block_size(frame, block_size=16):\n",
    "    h, w, _ = frame.shape\n",
    "    new_w = (w // block_size) * block_size\n",
    "    new_h = (h // block_size) * block_size\n",
    "    return cv2.resize(frame, (new_w, new_h))\n",
    "\n",
    "# Run the agent in the environment\n",
    "while step_count < max_steps and not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env_10_20.step(action)\n",
    "    frame = env_10_20.render()\n",
    "\n",
    "    # Resize the frame to avoid the macro_block_size warning\n",
    "    resized_frame = resize_frame_to_macro_block_size(frame)\n",
    "    frames.append(resized_frame)\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "# Close the environment\n",
    "env_10_20.close()\n",
    "\n",
    "# Save the frames as a video\n",
    "video_filename = \"velocity_study.mp4\"\n",
    "imageio.mimsave(video_filename, frames, fps=30)\n",
    "print(f\"Video saved as {video_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the video\n",
    "video_filename = \"velocity_study.mp4\"\n",
    "Video(video_filename, embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bii)",
   "language": "python",
   "name": "bii"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
