{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Study on the Ideal Behaviour for Merging into the Highway**\n",
    "\n",
    "##### This study aims to determine the optimal strategy for the ego vehicle to safely and efficiently merge onto a highway, prioritizing the action of braking to allow oncoming vehicles to pass. The variables under consideration are the reward for the braking action, which will be shaped based on how close the oncoming vehicle is, and an influence penalty, which penalizes in case the other vehicle behaviour changes due to the ego vehicle. The goal is to find the optimal reward configuration that encourages the ego vehicle to brake at the right moment, ensuring both safety and traffic efficiency and affect the least the behaviour of the other vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint\n",
    "import highway_env\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from highway_env import utils\n",
    "from highway_env.envs import MergeEnv\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creation of the environment**\n",
    "\n",
    "##### With the ego-vehicle on the merging lane and a single vehicle on the highway, on the right most lane and a costumized reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightLaneVehicle(ControlledVehicle):\n",
    "    \"\"\"\n",
    "    Um veículo que é restrito a ficar na lane da direita e nunca muda de lane.\n",
    "    \"\"\"\n",
    "    def act(self, action: int = None) -> None:\n",
    "        # Assegura que o veículo não mude de lane (desautoriza ações 0 e 2 para mudança de lane)\n",
    "        if action in [0, 2]:  # Ações para mudar para a esquerda ou direita\n",
    "            action = 1  # Forçar a manter a lane (ação 1)\n",
    "        super().act(action)\n",
    "\n",
    "\n",
    "class CustomMergeEnv(MergeEnv):\n",
    "    def _make_vehicles(self) -> None:\n",
    "        road = self.road\n",
    "\n",
    "        # Ponto de mesclagem (merge) na lane 0\n",
    "        merge_position = road.network.get_lane((\"b\", \"c\", 0)).position(0, 0)  # Ponto de mesclagem na autoestrada\n",
    "        \n",
    "        # Posição inicial do veículo ego na lane de mesclagem\n",
    "        ego_initial_position = road.network.get_lane((\"j\", \"k\", 0)).position(30, 0)  # Ego vehicle na lane de mesclagem\n",
    "\n",
    "        # Posição inicial do veículo da autoestrada na lane mais à direita (lane 1)\n",
    "        highway_vehicle_initial_position = road.network.get_lane((\"a\", \"b\", 1)).position(80, 0)  # Na lane 1 da autoestrada\n",
    "\n",
    "        # Definir velocidades iniciais\n",
    "        ego_speed = 20  # Velocidade inicial do ego\n",
    "        highway_speed = 30  # Velocidade inicial do veículo na autoestrada\n",
    "\n",
    "        # Calcular o tempo para ambos os veículos chegarem ao ponto de mesclagem\n",
    "        time_to_merge = (merge_position[0] - ego_initial_position[0]) / ego_speed\n",
    "\n",
    "        # Ajustar a velocidade do veículo da autoestrada para garantir que ambos cheguem ao mesmo tempo\n",
    "        highway_vehicle_speed = (merge_position[0] - highway_vehicle_initial_position[0]) / time_to_merge\n",
    "\n",
    "        # Criar o veículo ego na lane de mesclagem\n",
    "        ego_vehicle = self.action_type.vehicle_class(\n",
    "            road, ego_initial_position, speed=ego_speed\n",
    "        )\n",
    "        road.vehicles.append(ego_vehicle)\n",
    "\n",
    "        # Criar o veículo na lane da direita da autoestrada (lane 1)\n",
    "        highway_vehicle = RightLaneVehicle(\n",
    "            road, highway_vehicle_initial_position, speed=highway_vehicle_speed\n",
    "        )\n",
    "        road.vehicles.append(highway_vehicle)\n",
    "\n",
    "        # Definir o veículo ego como o veículo principal\n",
    "        self.vehicle = ego_vehicle\n",
    "\n",
    "        # Debug: Verificar posições e velocidades dos veículos\n",
    "        print(f\"Posição do veículo ego: {ego_vehicle.position}, Velocidade: {ego_vehicle.speed}\")\n",
    "        print(f\"Posição do veículo da autoestrada: {highway_vehicle.position}, Velocidade: {highway_vehicle.speed}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Custom reward function that incentivizes the ego vehicle to brake near the merging point \n",
    "        and let the highway vehicle pass before merging.\n",
    "        \"\"\"\n",
    "        # Get the original reward from the parent class (if it exists)\n",
    "        reward = super()._reward(action)\n",
    "        \n",
    "        ego_vehicle = self.vehicle\n",
    "        road = self.road\n",
    "\n",
    "        # Find the highway vehicle (vehicle in the rightmost lane)\n",
    "        highway_vehicle = None\n",
    "        for vehicle in road.vehicles:\n",
    "            if isinstance(vehicle, RightLaneVehicle):  # Identify the highway vehicle\n",
    "                highway_vehicle = vehicle\n",
    "                break\n",
    "        \n",
    "        if not highway_vehicle:\n",
    "            return reward\n",
    "\n",
    "        # Calculate relative positions and velocities\n",
    "        distance_to_highway_vehicle = highway_vehicle.position[0] - ego_vehicle.position[0]\n",
    "        is_ahead = distance_to_highway_vehicle > 0  # Check if the highway vehicle is ahead\n",
    "        near_merge_point = abs(ego_vehicle.position[0] - road.network.get_lane((\"b\", \"c\", 0)).position(0, 0)[0]) < 100\n",
    "\n",
    "        # Estimate acceleration based on change in speed\n",
    "        if not hasattr(self, \"_previous_speed\"):\n",
    "            self._previous_speed = ego_vehicle.speed  # Initialize previous speed\n",
    "\n",
    "        # Calculate acceleration as change in speed over time (assuming time step of 1)\n",
    "        acceleration = ego_vehicle.speed - self._previous_speed\n",
    "        self._previous_speed = ego_vehicle.speed  # Update for the next step\n",
    "\n",
    "        # Estimate acceleration of the highway vehicle\n",
    "        if not hasattr(self, \"_previous_highway_speed\"):\n",
    "            self._previous_highway_speed = highway_vehicle.speed  # Initialize previous speed\n",
    "\n",
    "        highway_acceleration = highway_vehicle.speed - self._previous_highway_speed\n",
    "        self._previous_highway_speed = highway_vehicle.speed  # Update for the next step\n",
    "\n",
    "        \n",
    "        # Reward for braking and letting the highway vehicle pass\n",
    "        braking_reward = 0.0\n",
    "        if near_merge_point and is_ahead:\n",
    "            # Ego vehicle should brake\n",
    "            if ego_vehicle.speed < highway_vehicle.speed and acceleration < 0:\n",
    "                braking_reward = self.config.get(\"braking_bonus\", 1.0)  # Incentive for braking\n",
    "            else:\n",
    "                braking_reward = -self.config.get(\"braking_penalty\", 1.0)\n",
    "            # Additional reward if ego vehicle stays behind the highway vehicle\n",
    "            if distance_to_highway_vehicle > 0 and ego_vehicle.speed < highway_vehicle.speed:\n",
    "                braking_reward += self.config.get(\"yielding_bonus\", 2.0)\n",
    "            else:\n",
    "                braking_reward -= self.config.get(\"yielding_penalty\", 2.0)\n",
    "        \n",
    "        # Penalize interference with the highway vehicle\n",
    "        influence_penalty = 0.0\n",
    "        if near_merge_point and distance_to_highway_vehicle < 20:  # Close to the highway vehicle\n",
    "            if highway_acceleration < -1.0:  # Significant deceleration detected\n",
    "                influence_penalty = self.config.get(\"influence_penalty\", 5.0)  # Large penalty for interference\n",
    "\n",
    "\n",
    "        # Total reward includes the braking incentive and interference penalty\n",
    "        reward += braking_reward - influence_penalty\n",
    "\n",
    "        # Debug information\n",
    "        print(f\"Distance to highway vehicle: {distance_to_highway_vehicle}, Ego speed: {ego_vehicle.speed}, Highway speed: {highway_vehicle.speed}, Highway acceleration: {highway_acceleration}\")\n",
    "        print(f\"Braking reward: {braking_reward}, Influence penalty: {influence_penalty}, Total reward: {reward}\")\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment CustomMerge-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Registering the custom environment\n",
    "gym.envs.registration.register(\n",
    "    id='CustomMerge-v0',\n",
    "    entry_point='__main__:CustomMergeEnv',  # Entry point for your custom environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posição do veículo ego: [30.  14.5], Velocidade: 20\n",
      "Posição do veículo da autoestrada: [80.  4.], Velocidade: 15.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CustomMerge-v0\", render_mode='rgb_array')\n",
    "pprint.pprint(env.unwrapped.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posição do veículo ego: [30.  14.5], Velocidade: 20\n",
      "Posição do veículo da autoestrada: [80.  4.], Velocidade: 15.0\n",
      "True\n",
      "False\n",
      "Distance to highway vehicle: 44.99999999999997, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "False\n",
      "Distance to highway vehicle: 39.99999999999997, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "False\n",
      "Distance to highway vehicle: 35.00000000000004, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "False\n",
      "Distance to highway vehicle: 30.000000000000114, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "False\n",
      "Distance to highway vehicle: 25.00000000000017, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "1\n",
      "3\n",
      "Distance to highway vehicle: 20.0000035883603, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "1\n",
      "3\n",
      "Distance to highway vehicle: 15.028568981385575, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "1\n",
      "3\n",
      "Distance to highway vehicle: 10.159787043440048, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "1\n",
      "3\n",
      "Distance to highway vehicle: 5.29332351732026, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "1\n",
      "3\n",
      "Distance to highway vehicle: 0.3245020104458831, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "True\n",
      "Distance to highway vehicle: -4.67547764465445, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.9444444444444444\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "True\n",
      "Distance to highway vehicle: -9.675476849382733, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.9444444444444444\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "True\n",
      "Distance to highway vehicle: -14.675476839130681, Ego speed: 20.0, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.9444444444444444\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "True\n",
      "Distance to highway vehicle: -16.49999878769586, Ego speed: 17.422222222222224, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.3244444444444445\n",
      "crashTrue\n",
      "overFalse\n",
      "False\n",
      "True\n",
      "Distance to highway vehicle: -1.4999987876959153, Ego speed: 6.189494740697979, Highway speed: 15.0\n",
      "Braking reward: 0.0, Total reward: 0.043626257406338284\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 13.500001212304085, Ego speed: 2.1989069279729057, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.9438615620882116\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 28.500001212304085, Ego speed: 0.781193276745879, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.9084187208075356\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 43.500001212304085, Ego speed: 0.2775301345726095, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.895827142253204\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 58.500001212304085, Ego speed: 0.09859656744197272, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.891353803074938\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 73.50000121230408, Ego speed: 0.035027847070769606, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.889764585065658\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 88.50000121230408, Ego speed: 0.012444145899250726, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.88919999253637\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 103.50000121230408, Ego speed: 0.004420961609458013, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.888999412929125\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 118.50000121230408, Ego speed: 0.0015706101254790333, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.8889281541420258\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 133.50000121230408, Ego speed: 0.0005579818112375971, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.88890283843417\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 148.50000121230408, Ego speed: 0.00019823105468458013, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.888893844665256\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 163.50000121230408, Ego speed: 7.042443006198341e-05, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.8888906494996403\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 178.50000121230408, Ego speed: 2.5019290531681702e-05, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.888889514371152\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 193.50000121230408, Ego speed: 8.888462400870838e-06, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.888889111100449\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 208.50000121230408, Ego speed: 3.1577539639523984e-06, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.888888967832738\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 223.50000121230408, Ego speed: 1.1218374615479217e-06, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.8888889169348255\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 238.50000121230408, Ego speed: 3.9854887508622126e-07, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.8888888988526107\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 253.50000121230408, Ego speed: 1.4159021362446023e-07, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.888888892428644\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 268.5000012123041, Ego speed: 5.0301957545064417e-08, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.888888890146438\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 283.5000012123041, Ego speed: 1.7870493080662645e-08, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.8888888893356515\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 298.5000012123041, Ego speed: 6.348749403239602e-09, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.8888888890476077\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "True\n",
      "near and ahead\n",
      "0\n",
      "2\n",
      "Distance to highway vehicle: 313.5000012123041, Ego speed: 2.255484434771993e-09, Highway speed: 15.0\n",
      "Braking reward: 3.0, Total reward: 2.8888888889452757\n",
      "crashTrue\n",
      "overFalse\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mactions_indexes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDLE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\highway_env\\envs\\common\\abstract.py:305\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 305\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m()\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_image'"
     ]
    }
   ],
   "source": [
    "# Para testar se o ambiente está correto\n",
    "# env.reset()\n",
    "# for _ in range(100):\n",
    "#     action = env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     env.render()\n",
    "\n",
    "# plt.imshow(env.render())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the models for several rewards**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial configuration with balanced values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.config.update({\n",
    "    \"braking_bonus\": 1.0,\n",
    "    \"braking_penalty\": 1.0,\n",
    "    \"yielding_bonus\": 2.0,\n",
    "    \"yielding_penalty\": 2.0,\n",
    "    \"influence_penalty\": 5.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env,\n",
    "#             policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#             learning_rate=5e-4,\n",
    "#             n_steps=2048, \n",
    "#             batch_size=64, \n",
    "#             n_epochs=10,  \n",
    "#             gamma=0.8,\n",
    "#             gae_lambda=0.95, \n",
    "#             clip_range=0.2, \n",
    "#             verbose=1,\n",
    "#             tensorboard_log=\"env_ego_entering_brake_close_0/\")\n",
    "# timesteps = 50000\n",
    "# model.learn(total_timesteps=timesteps)\n",
    "# model.save(\"env_ego_entering_brake_close_0/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration to force the ego vehicle to let the highway vehicle go ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.config.update({\n",
    "    \"braking_bonus\": 1.0,\n",
    "    \"braking_penalty\": 1.0,\n",
    "    \"yielding_bonus\": 4.0,\n",
    "    \"yielding_penalty\": 2.0,\n",
    "    \"influence_penalty\": 5.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env,\n",
    "#             policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#             learning_rate=5e-4,\n",
    "#             n_steps=2048, \n",
    "#             batch_size=64, \n",
    "#             n_epochs=10,  \n",
    "#             gamma=0.8,\n",
    "#             gae_lambda=0.95, \n",
    "#             clip_range=0.2, \n",
    "#             verbose=1,\n",
    "#             tensorboard_log=\"env_ego_entering_brake_close_1/\")\n",
    "# timesteps = 50000\n",
    "# model.learn(total_timesteps=timesteps)\n",
    "# model.save(\"env_ego_entering_brake_close_1/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration to severally punish influences on the highway vehicle behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.config.update({\n",
    "    \"braking_bonus\": 1.0,\n",
    "    \"braking_penalty\": 1.0,\n",
    "    \"yielding_bonus\": 2.0,\n",
    "    \"yielding_penalty\": 2.0,\n",
    "    \"influence_penalty\": 10.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env,\n",
    "#             policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#             learning_rate=5e-4,\n",
    "#             n_steps=2048, \n",
    "#             batch_size=64, \n",
    "#             n_epochs=10,  \n",
    "#             gamma=0.8,\n",
    "#             gae_lambda=0.95, \n",
    "#             clip_range=0.2, \n",
    "#             verbose=1,\n",
    "#             tensorboard_log=\"env_ego_entering_brake_close_2/\")\n",
    "# timesteps = 50000\n",
    "# model.learn(total_timesteps=timesteps)\n",
    "# model.save(\"env_ego_entering_brake_close_2/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Safe\" configuration - increase the yielding_bonus and the influence_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.config.update({\n",
    "    \"braking_bonus\": 1.0,\n",
    "    \"braking_penalty\": 1.0,\n",
    "    \"yielding_bonus\": 4.0,\n",
    "    \"yielding_penalty\": 2.0,\n",
    "    \"influence_penalty\": 10.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env,\n",
    "#             policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#             learning_rate=5e-4,\n",
    "#             n_steps=2048, \n",
    "#             batch_size=64, \n",
    "#             n_epochs=10,  \n",
    "#             gamma=0.8,\n",
    "#             gae_lambda=0.95, \n",
    "#             clip_range=0.2, \n",
    "#             verbose=1,\n",
    "#             tensorboard_log=\"env_ego_entering_brake_close_3/\")\n",
    "# timesteps = 50000\n",
    "# model.learn(total_timesteps=timesteps)\n",
    "# model.save(\"env_ego_entering_brake_close_3/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Agressive\" configuration - Reduce yielding_bonus and increase the braking_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.config.update({\n",
    "    \"braking_bonus\": 4.0,\n",
    "    \"braking_penalty\": 1.0,\n",
    "    \"yielding_bonus\": 0.5,\n",
    "    \"yielding_penalty\": 2.0,\n",
    "    \"influence_penalty\": 5.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env,\n",
    "#             policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#             learning_rate=5e-4,\n",
    "#             n_steps=2048, \n",
    "#             batch_size=64, \n",
    "#             n_epochs=10,  \n",
    "#             gamma=0.8,\n",
    "#             gae_lambda=0.95, \n",
    "#             clip_range=0.2, \n",
    "#             verbose=1,\n",
    "#             tensorboard_log=\"env_ego_entering_brake_close_4/\")\n",
    "# timesteps = 50000\n",
    "# model.learn(total_timesteps=timesteps)\n",
    "# model.save(\"env_ego_entering_brake_close_4/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
