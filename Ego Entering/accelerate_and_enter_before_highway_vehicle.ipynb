{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Study on the Ideal Behaviour for Merging into the Highway**\n",
    "\n",
    "##### This study aims to determine the optimal strategy for the ego vehicle to safely and efficiently merge onto a highway, prioritizing the action of braking to allow oncoming vehicles to pass. The only variable under consideration is the reward for the braking action, which will be shaped based on how close the oncoming vehicle is. The goal is to find the optimal reward configuration that encourages the ego vehicle to brake at the right moment, ensuring both safety and traffic efficiency. This reward will be progressively fine-tuned to determine the best braking behavior, either identifying a single optimal strategy or a range of effective solutions depending on the proximity of the approaching vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint\n",
    "import highway_env\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from highway_env import utils\n",
    "from highway_env.envs import MergeEnv\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creation of the environment**\n",
    "\n",
    "##### With the ego-vehicle on the merging lane and a single vehicle on the highway, on the right most lane and a costumized reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightLaneVehicle(ControlledVehicle):\n",
    "    \"\"\"\n",
    "    Um veículo que é restrito a ficar na lane da direita e nunca muda de lane.\n",
    "    \"\"\"\n",
    "    def act(self, action: int = None) -> None:\n",
    "        # Assegura que o veículo não mude de lane (desautoriza ações 0 e 2 para mudança de lane)\n",
    "        if action in [0, 2]:  # Ações para mudar para a esquerda ou direita\n",
    "            action = 1  # Forçar a manter a lane (ação 1)\n",
    "        super().act(action)\n",
    "\n",
    "\n",
    "class CustomMergeEnv(MergeEnv):\n",
    "    def _make_vehicles(self) -> None:\n",
    "        road = self.road\n",
    "\n",
    "        # Ponto de mesclagem (merge) na lane 0\n",
    "        merge_position = road.network.get_lane((\"b\", \"c\", 0)).position(0, 0)  # Ponto de mesclagem na autoestrada\n",
    "        \n",
    "        # Posição inicial do veículo ego na lane de mesclagem\n",
    "        ego_initial_position = road.network.get_lane((\"j\", \"k\", 0)).position(30, 0)  # Ego vehicle na lane de mesclagem\n",
    "\n",
    "        # Posição inicial do veículo da autoestrada na lane mais à direita (lane 1)\n",
    "        highway_vehicle_initial_position = road.network.get_lane((\"a\", \"b\", 1)).position(80, 0)  # Na lane 1 da autoestrada\n",
    "\n",
    "        # Definir velocidades iniciais\n",
    "        ego_speed = 20  # Velocidade inicial do ego\n",
    "        highway_speed = 30  # Velocidade inicial do veículo na autoestrada\n",
    "\n",
    "        # Calcular o tempo para ambos os veículos chegarem ao ponto de mesclagem\n",
    "        time_to_merge = (merge_position[0] - ego_initial_position[0]) / ego_speed\n",
    "\n",
    "        # Ajustar a velocidade do veículo da autoestrada para garantir que ambos cheguem ao mesmo tempo\n",
    "        highway_vehicle_speed = (merge_position[0] - highway_vehicle_initial_position[0]) / time_to_merge\n",
    "\n",
    "        # Criar o veículo ego na lane de mesclagem\n",
    "        ego_vehicle = self.action_type.vehicle_class(\n",
    "            road, ego_initial_position, speed=ego_speed\n",
    "        )\n",
    "        road.vehicles.append(ego_vehicle)\n",
    "\n",
    "        # Criar o veículo na lane da direita da autoestrada (lane 1)\n",
    "        highway_vehicle = RightLaneVehicle(\n",
    "            road, highway_vehicle_initial_position, speed=highway_vehicle_speed\n",
    "        )\n",
    "        road.vehicles.append(highway_vehicle)\n",
    "\n",
    "        # Definir o veículo ego como o veículo principal\n",
    "        self.vehicle = ego_vehicle\n",
    "\n",
    "        # Debug: Verificar posições e velocidades dos veículos\n",
    "        print(f\"Posição do veículo ego: {ego_vehicle.position}, Velocidade: {ego_vehicle.speed}\")\n",
    "        print(f\"Posição do veículo da autoestrada: {highway_vehicle.position}, Velocidade: {highway_vehicle.speed}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Custom reward function that incentivizes the ego vehicle to accelerate and merge onto the highway\n",
    "        ahead of the highway vehicle.\n",
    "        \"\"\"\n",
    "        # Get the original reward from the parent class (if it exists)\n",
    "        reward = super()._reward(action)\n",
    "        \n",
    "        ego_vehicle = self.vehicle\n",
    "        road = self.road\n",
    "\n",
    "        # Find the highway vehicle (vehicle in the rightmost lane)\n",
    "        highway_vehicle = None\n",
    "        for vehicle in road.vehicles:\n",
    "            if isinstance(vehicle, RightLaneVehicle):  # Identify the highway vehicle\n",
    "                highway_vehicle = vehicle\n",
    "                break\n",
    "\n",
    "        if not highway_vehicle:\n",
    "            return reward\n",
    "\n",
    "        # Calculate relative positions and velocities\n",
    "        distance_to_highway_vehicle = highway_vehicle.position[0] - ego_vehicle.position[0]\n",
    "        # is_ahead = distance_to_highway_vehicle > 0  # Check if the highway vehicle is ahead\n",
    "        near_merge_point = abs(ego_vehicle.position[0] - road.network.get_lane((\"b\", \"c\", 0)).position(0, 0)[0]) < 100\n",
    "        print(near_merge_point)\n",
    "        # Estimate acceleration based on change in speed\n",
    "        if not hasattr(self, \"_previous_speed\"):\n",
    "            self._previous_speed = ego_vehicle.speed  # Initialize previous speed\n",
    "\n",
    "        # Calculate acceleration as change in speed over time (assuming time step of 1)\n",
    "        acceleration = ego_vehicle.speed - self._previous_speed\n",
    "        self._previous_speed = ego_vehicle.speed  # Update for the next step\n",
    "\n",
    "        # Reward for accelerating and merging ahead of the highway vehicle\n",
    "        merging_reward = 0.0\n",
    "        if near_merge_point:\n",
    "            print(\"Near merge point\")\n",
    "            # Reward ego vehicle for accelerating\n",
    "            if ego_vehicle.speed > highway_vehicle.speed and acceleration > 0:\n",
    "                print(\"Accelerating successfully\")\n",
    "                merging_reward = self.config.get(\"acceleration_bonus\", 1.5)  # Incentive for accelerating\n",
    "            else:\n",
    "                print(\"Not accelerating enough\")\n",
    "                merging_reward -= self.config.get(\"acceleration_penalty\", 1.0)  # Penalty for not accelerating\n",
    "            \n",
    "            # Additional reward if ego vehicle successfully gets ahead of the highway vehicle\n",
    "            if distance_to_highway_vehicle < 0:  # Ego vehicle is ahead of the highway vehicle\n",
    "                print(\"Successfully merged ahead\")\n",
    "                merging_reward += self.config.get(\"merging_bonus\", 3.0)\n",
    "            else:\n",
    "                print(\"Failed to merge ahead\")\n",
    "                merging_reward -= self.config.get(\"merging_penalty\", 2.0)\n",
    "\n",
    "        # Total reward includes the merging incentive\n",
    "        reward += merging_reward\n",
    "\n",
    "        # Debug information\n",
    "        print(f\"Distance to highway vehicle: {distance_to_highway_vehicle}, Ego speed: {ego_vehicle.speed}, Highway speed: {highway_vehicle.speed}\")\n",
    "        print(f\"Merging reward: {merging_reward}, Total reward: {reward}\")\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment CustomMerge-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Registering the custom environment\n",
    "gym.envs.registration.register(\n",
    "    id='CustomMerge-v0',\n",
    "    entry_point='__main__:CustomMergeEnv',  # Entry point for your custom environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posição do veículo ego: [30.  14.5], Velocidade: 20\n",
      "Posição do veículo da autoestrada: [80.  4.], Velocidade: 15.0\n",
      "{'action': {'type': 'DiscreteMetaAction'},\n",
      " 'centering_position': [0.3, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'high_speed_reward': 0.2,\n",
      " 'lane_change_reward': -0.05,\n",
      " 'manual_control': False,\n",
      " 'merging_speed_reward': -0.5,\n",
      " 'observation': {'type': 'Kinematics'},\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 1,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'reward_speed_range': [20, 30],\n",
      " 'right_lane_reward': 0.1,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 150,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CustomMerge-v0\", render_mode='rgb_array')\n",
    "pprint.pprint(env.unwrapped.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posição do veículo ego: [30.  14.5], Velocidade: 20\n",
      "Posição do veículo da autoestrada: [80.  4.], Velocidade: 15.0\n",
      "False\n",
      "Distance to highway vehicle: 44.99999999999997, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "Distance to highway vehicle: 39.99999999999997, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "Distance to highway vehicle: 35.00000000000004, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "Distance to highway vehicle: 30.000000000000114, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "False\n",
      "Distance to highway vehicle: 25.00000000000017, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 0.0, Total reward: 0.8333333333333333\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 20.0000035883603, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 15.028568981385575, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 10.159787043440048, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 5.29332351732026, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 0.3245020104458831, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -2.166666666666667\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Successfully merged ahead\n",
      "Distance to highway vehicle: -4.67547764465445, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 2.0, Total reward: 2.9444444444444446\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Successfully merged ahead\n",
      "Distance to highway vehicle: -9.675476849382733, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 2.0, Total reward: 2.9444444444444446\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Successfully merged ahead\n",
      "Distance to highway vehicle: -14.675476839130681, Ego speed: 20.0, Highway speed: 15.0\n",
      "Merging reward: 2.0, Total reward: 2.9444444444444446\n",
      "crashFalse\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Successfully merged ahead\n",
      "Distance to highway vehicle: -16.49999878769586, Ego speed: 17.422222222222224, Highway speed: 15.0\n",
      "Merging reward: 2.0, Total reward: 2.3244444444444445\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Successfully merged ahead\n",
      "Distance to highway vehicle: -1.4999987876959153, Ego speed: 6.189494740697979, Highway speed: 15.0\n",
      "Merging reward: 2.0, Total reward: 2.043626257406338\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 13.500001212304085, Ego speed: 2.1989069279729057, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -3.0561384379117884\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 28.500001212304085, Ego speed: 0.781193276745879, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -3.0915812791924644\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 43.500001212304085, Ego speed: 0.2775301345726095, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -3.104172857746796\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 58.500001212304085, Ego speed: 0.09859656744197272, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -3.108646196925062\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 73.50000121230408, Ego speed: 0.035027847070769606, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -3.110235414934342\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 88.50000121230408, Ego speed: 0.012444145899250726, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -3.11080000746363\n",
      "crashTrue\n",
      "overFalse\n",
      "True\n",
      "Near merge point\n",
      "Not accelerating enough\n",
      "Failed to merge ahead\n",
      "Distance to highway vehicle: 103.50000121230408, Ego speed: 0.004420961609458013, Highway speed: 15.0\n",
      "Merging reward: -3.0, Total reward: -3.111000587070875\n",
      "crashTrue\n",
      "overFalse\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mactions_indexes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDLE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\highway_env\\envs\\common\\abstract.py:305\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 305\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m()\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_image'"
     ]
    }
   ],
   "source": [
    "# Para testar se o ambiente está correto\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO('MlpPolicy', env,\n",
    "#             policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#             learning_rate=5e-4,\n",
    "#             n_steps=2048, \n",
    "#             batch_size=64, \n",
    "#             n_epochs=10,  \n",
    "#             gamma=0.8,\n",
    "#             gae_lambda=0.95, \n",
    "#             clip_range=0.2, \n",
    "#             verbose=1,\n",
    "#             tensorboard_log=\"env_ego_entering_brake_close/\")\n",
    "# timesteps = 50000\n",
    "# model.learn(total_timesteps=timesteps)\n",
    "# model.save(\"env_ego_entering_brake_close/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
