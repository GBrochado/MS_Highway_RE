{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Study on the Ideal Behaviour for Merging into the Highway. Access the influence on other vehicles.**\n",
    "\n",
    "##### This study aims to determine the optimal strategy for the ego vehicle to safely and efficiently merge onto a highway, prioritizing the action of accelerating and merge before the other vehicle reaches the merging point. The only variables under consideration are the **reward for merging before the highway vehicle reaches the merging point** and the **influence reward**, which penalizes if the actions of the ego vehicle significantly influences the highway vehicle behaviour. The goal is to find the optimal reward configuration that encourages the ego vehicle to accelerate, ensuring both safety and traffic efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import cv2\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint\n",
    "import highway_env\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from highway_env import utils\n",
    "from highway_env.envs import MergeEnv\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creation of the environment**\n",
    "\n",
    "##### With the ego-vehicle on the merging lane and a single vehicle on the highway, on the right most lane and a costumized reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightLaneVehicle(ControlledVehicle):\n",
    "    \"\"\"\n",
    "    Um veículo que é restrito a ficar na lane da direita e nunca muda de lane.\n",
    "    \"\"\"\n",
    "    def act(self, action: int = None) -> None:\n",
    "        # Assegura que o veículo não mude de lane (desautoriza ações 0 e 2 para mudança de lane)\n",
    "        if action in [0, 2]:  # Ações para mudar para a esquerda ou direita\n",
    "            action = 1  # Forçar a manter a lane (ação 1)\n",
    "        super().act(action)\n",
    "\n",
    "\n",
    "class CustomMergeEnv(MergeEnv):\n",
    "    def _make_vehicles(self) -> None:\n",
    "        road = self.road\n",
    "\n",
    "        # Ponto de mesclagem (merge) na lane 0\n",
    "        merge_position = road.network.get_lane((\"b\", \"c\", 0)).position(0, 0)  # Ponto de mesclagem na autoestrada\n",
    "        \n",
    "        # Posição inicial do veículo ego na lane de mesclagem\n",
    "        ego_initial_position = road.network.get_lane((\"j\", \"k\", 0)).position(30, 0)  # Ego vehicle na lane de mesclagem\n",
    "\n",
    "        # Posição inicial do veículo da autoestrada na lane mais à direita (lane 1)\n",
    "        highway_vehicle_initial_position = road.network.get_lane((\"a\", \"b\", 1)).position(80, 0)  # Na lane 1 da autoestrada\n",
    "\n",
    "        # Definir velocidades iniciais\n",
    "        ego_speed = 20  # Velocidade inicial do ego\n",
    "        highway_speed = 30  # Velocidade inicial do veículo na autoestrada\n",
    "\n",
    "        # Calcular o tempo para ambos os veículos chegarem ao ponto de mesclagem\n",
    "        time_to_merge = (merge_position[0] - ego_initial_position[0]) / ego_speed\n",
    "\n",
    "        # Ajustar a velocidade do veículo da autoestrada para garantir que ambos cheguem ao mesmo tempo\n",
    "        highway_vehicle_speed = (merge_position[0] - highway_vehicle_initial_position[0]) / time_to_merge\n",
    "\n",
    "        # Criar o veículo ego na lane de mesclagem\n",
    "        ego_vehicle = self.action_type.vehicle_class(\n",
    "            road, ego_initial_position, speed=ego_speed\n",
    "        )\n",
    "        road.vehicles.append(ego_vehicle)\n",
    "\n",
    "        # Criar o veículo na lane da direita da autoestrada (lane 1)\n",
    "        highway_vehicle = RightLaneVehicle(\n",
    "            road, highway_vehicle_initial_position, speed=highway_vehicle_speed\n",
    "        )\n",
    "        road.vehicles.append(highway_vehicle)\n",
    "\n",
    "        # Definir o veículo ego como o veículo principal\n",
    "        self.vehicle = ego_vehicle\n",
    "\n",
    "        # Debug: Verificar posições e velocidades dos veículos\n",
    "        print(f\"Posição do veículo ego: {ego_vehicle.position}, Velocidade: {ego_vehicle.speed}\")\n",
    "        print(f\"Posição do veículo da autoestrada: {highway_vehicle.position}, Velocidade: {highway_vehicle.speed}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Custom reward function that penalizes the ego vehicle if its actions influence the highway vehicle,\n",
    "        while incentivizing efficient merging behavior.\n",
    "        \"\"\"\n",
    "        # Get the original reward from the parent class (if it exists)\n",
    "        reward = super()._reward(action)\n",
    "        \n",
    "        ego_vehicle = self.vehicle\n",
    "        road = self.road\n",
    "\n",
    "        # Find the highway vehicle (vehicle in the rightmost lane)\n",
    "        highway_vehicle = None\n",
    "        for vehicle in road.vehicles:\n",
    "            if isinstance(vehicle, RightLaneVehicle):  # Identify the highway vehicle\n",
    "                highway_vehicle = vehicle\n",
    "                break\n",
    "\n",
    "        if not highway_vehicle:\n",
    "            return reward\n",
    "\n",
    "        # Calculate relative positions and velocities\n",
    "        distance_to_highway_vehicle = abs(highway_vehicle.position[0] - ego_vehicle.position[0])\n",
    "        # is_ahead = distance_to_highway_vehicle > 0  # Check if the highway vehicle is ahead\n",
    "        near_merge_point = abs(ego_vehicle.position[0] - road.network.get_lane((\"b\", \"c\", 0)).position(0, 0)[0]) < 100\n",
    "\n",
    "        # Estimate highway vehicle's deceleration based on change in speed\n",
    "        if not hasattr(self, \"_previous_highway_speed\"):\n",
    "            self._previous_highway_speed = highway_vehicle.speed  # Initialize previous speed\n",
    "\n",
    "        highway_acceleration = highway_vehicle.speed - self._previous_highway_speed\n",
    "        self._previous_highway_speed = highway_vehicle.speed  # Update for the next step\n",
    "\n",
    "        # Penalize ego vehicle for influencing the highway vehicle's behavior\n",
    "        influence_penalty = 0.0\n",
    "        if near_merge_point and distance_to_highway_vehicle < 20:  # Close to the highway vehicle\n",
    "            if highway_acceleration < -1.0:  # Significant deceleration (tunable threshold)\n",
    "                print(\"Highway vehicle influenced: significant deceleration detected\")\n",
    "                influence_penalty = self.config.get(\"influence_penalty\", 5.0)  # Large penalty for interference\n",
    "\n",
    "        # Reward for merging efficiently\n",
    "        merging_reward = 0.0\n",
    "        if near_merge_point:\n",
    "            if ego_vehicle.speed > highway_vehicle.speed and distance_to_highway_vehicle < 0:\n",
    "                merging_reward += self.config.get(\"merging_bonus\", 3.0)\n",
    "            else:\n",
    "                merging_reward -= self.config.get(\"merging_penalty\", 2.0)\n",
    "\n",
    "        # Total reward includes the merging incentive and interference penalty\n",
    "        reward += merging_reward - influence_penalty\n",
    "\n",
    "        # Debug information\n",
    "        print(f\"Distance to highway vehicle: {distance_to_highway_vehicle}, Ego speed: {ego_vehicle.speed}, Highway speed: {highway_vehicle.speed}, Highway acceleration: {highway_acceleration}\")\n",
    "        print(f\"Merging reward: {merging_reward}, Influence penalty: {influence_penalty}, Total reward: {reward}\")\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the custom environment\n",
    "gym.envs.registration.register(\n",
    "    id='CustomMerge-v0',\n",
    "    entry_point='__main__:CustomMergeEnv',  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial configuration with balanced values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posição do veículo ego: [30.  14.5], Velocidade: 20\n",
      "Posição do veículo da autoestrada: [80.  4.], Velocidade: 15.0\n"
     ]
    }
   ],
   "source": [
    "env_v0 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array')\n",
    "env_v0.unwrapped.config.update({\n",
    "    \"influence_penalty\": 5.0,\n",
    "    \"merging_bonus\": 3.0,\n",
    "    \"merging_penalty\": 2.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_v0,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_ego_entering_accelerate_0/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_ego_entering_accelerate_0/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations with a severe increase on the penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_v1 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array')\n",
    "env_v1.unwrapped.config.update({\n",
    "    \"influence_penalty\": 10.0,\n",
    "    \"merging_bonus\": 1.5,\n",
    "    \"merging_penalty\": 2.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_v1,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_ego_entering_accelerate_1/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_ego_entering_accelerate_1/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_v2 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array')\n",
    "env_v2.unwrapped.config.update({\n",
    "    \"influence_penalty\": 5.0,\n",
    "    \"merging_bonus\": 1.5,\n",
    "    \"merging_penalty\": 4.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_v2,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_ego_entering_accelerate_2/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_ego_entering_accelerate_2/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_v3 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array')\n",
    "env_v3.unwrapped.config.update({\n",
    "    \"influence_penalty\": 10.0,\n",
    "    \"merging_bonus\": 1.5,\n",
    "    \"merging_penalty\": 4.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_v3,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_ego_entering_accelerate_3/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_ego_entering_accelerate_3/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration to incentivise the merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_v4 = gym.make(\"CustomMerge-v0\", render_mode='rgb_array')\n",
    "env_v4.unwrapped.config.update({\n",
    "    \"influence_penalty\": 3.0,\n",
    "    \"merging_bonus\": 5.0,\n",
    "    \"merging_penalty\": 2.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_v4,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_ego_entering_accelerate_4/\")\n",
    "timesteps = 1000000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_ego_entering_accelerate_4/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluate and compare the models**\n",
    "\n",
    "**For env_v0**\n",
    "- Average Reward:\n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For env_v1**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For env_v2**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions:\n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For env_v3**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): \n",
    "\n",
    "**For env_v4**\n",
    "- Average Reward: \n",
    "- Average Steps to Merge: \n",
    "- Average Episode Time: \n",
    "- Number of Collisions: \n",
    "- Successful Merges: \n",
    "- Number of Dangerous Driving Episodes (sudden speed changes): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(model, env, num_episodes, speed_threshold_ratio=0.5):\n",
    "    total_rewards = []  # List to store total rewards for each episode\n",
    "    total_collisions = 0  # Counter for total collisions across all episodes\n",
    "    successful_merges = 0  # Counter for successful merges\n",
    "    dangerous_driving_episodes = 0  # Counter for episodes with dangerous driving behavior\n",
    "    total_steps_to_merge = []  # List to store the number of steps taken to merge in each episode\n",
    "    total_episode_times = []  # List to store the time taken for each episode\n",
    "\n",
    "    # Cálculo do threshold de velocidade\n",
    "    reward_speed_range = env.unwrapped.config[\"reward_speed_range\"]\n",
    "    speed_threshold = (reward_speed_range[1] - reward_speed_range[0]) * speed_threshold_ratio  # Limite para mudanças repentinas de velocidade\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        start_time = time.time()  # Record the start time of the episode\n",
    "        obs, info = env.reset()  # Reset the environment and get the initial observation\n",
    "        done = False  # Variable to track if the episode is finished\n",
    "        episode_reward = 0  # Variable to track the reward for the current episode\n",
    "        collisions = 0  # Counter for collisions in the current episode\n",
    "        dangerous_driving = False  # Flag to indicate if dangerous driving occurred\n",
    "        steps_to_merge = 0  # Counter for steps taken to merge\n",
    "        last_speed = None  # Initialize last speed as None\n",
    "\n",
    "        # Armazenar as posições dos veículos na rodovia para verificar a fusão\n",
    "        highway_vehicles = []\n",
    "        for vehicle in env.road.vehicles:\n",
    "            # Verifica se o veículo não é o ego vehicle\n",
    "            if vehicle != env.vehicle:\n",
    "                highway_vehicles.append(vehicle)\n",
    "\n",
    "        while not done:  # Loop until the episode is done\n",
    "            # The agent chooses an action\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            # Execute the action in the environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            episode_reward += reward  # Accumulate reward for the episode\n",
    "            steps_to_merge += 1  # Increment the steps to merge counter\n",
    "\n",
    "            # Check the current speed and round to 2 decimal places\n",
    "            current_speed = round(info.get('speed', 0), 2)\n",
    "\n",
    "            # Check for sudden speed changes\n",
    "            if last_speed is not None and abs(current_speed - last_speed) > speed_threshold:\n",
    "                dangerous_driving = True  # Mark as dangerous driving if speed change exceeds threshold\n",
    "\n",
    "            last_speed = current_speed  # Update the last speed for the next iteration\n",
    "\n",
    "            # Check for collisions\n",
    "            if 'crashed' in info and info['crashed']:\n",
    "                collisions += 1  # Increment collision counter if a crash occurred\n",
    "\n",
    "            # Check if the episode has ended (either 'terminated' or 'truncated')\n",
    "            done = terminated or truncated\n",
    "\n",
    "            ego_position = env.vehicle.position[0]  # Get the position of the ego vehicle\n",
    "            highway_vehicles_positions = [vehicle.position[0] for vehicle in highway_vehicles]  # Get positions of highway vehicles\n",
    "            for highway_position in highway_vehicles_positions:\n",
    "                    if not collisions and ego_position > highway_position:\n",
    "                        successful_merges += 1  # Increment successful merges if the ego vehicle is ahead of at least one highway vehicle\n",
    "                        done = True  # End the episode if the merge is successful\n",
    "\n",
    "        # Log episode metrics\n",
    "        total_rewards.append(episode_reward)  # Add episode reward to the total rewards list\n",
    "        total_collisions += collisions  # Update total collisions count\n",
    "        total_steps_to_merge.append(steps_to_merge)  # Add steps to merge for this episode\n",
    "\n",
    "        if dangerous_driving:\n",
    "            dangerous_driving_episodes += 1  # Increment count of dangerous driving episodes\n",
    "\n",
    "        # Calculate the time taken for the episode and add to the list\n",
    "        episode_time = time.time() - start_time  # Calculate elapsed time\n",
    "        total_episode_times.append(episode_time)  # Add episode time to the list\n",
    "\n",
    "    # Final metric calculations\n",
    "    avg_reward = np.mean(total_rewards)  # Calculate average reward\n",
    "    avg_steps_to_merge = np.mean(total_steps_to_merge)  # Calculate average steps to merge\n",
    "    avg_episode_time = np.mean(total_episode_times)  # Calculate average episode time\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Average Reward: {avg_reward}\")  # Print average reward\n",
    "    print(f\"Average Steps to Merge: {avg_steps_to_merge}\")  # Print average steps to merge\n",
    "    print(f\"Average Episode Time: {avg_episode_time:.2f} seconds\")  # Print average episode time\n",
    "    print(f\"Number of Collisions: {total_collisions}\")  # Print total collisions\n",
    "    print(f\"Successful Merges: {successful_merges}\")  # Print number of successful merges\n",
    "    print(f\"Number of Dangerous Driving Episodes (sudden speed changes): {dangerous_driving_episodes}\")  # Print count of dangerous driving episodes\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": avg_reward,  # Return average reward\n",
    "        \"avg_steps_to_merge\": avg_steps_to_merge,  # Return average steps to merge\n",
    "        \"avg_episode_time\": avg_episode_time,  # Return average episode time\n",
    "        \"number_collisions\": total_collisions,  # Return total number of collisions\n",
    "        \"successful_merges\": successful_merges,  # Return number of successful merges\n",
    "        \"number_dangerous_episodes\": dangerous_driving_episodes  # Return number of dangerous driving episodes\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_ego_entering_accelerate_0/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_v0, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_ego_entering_accelerate_1/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_v1, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_ego_entering_accelerate_2/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_v2, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_ego_entering_accelerate_3/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_v3, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"env_ego_entering_accelerate_4/model\")  \n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_agent(model, env_v4, 200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXTINHO A ANALISAR OS RESULTADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUDAR PARA SER O VIDEO DO QUE TIVER MELHOR RESULTADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"vel_study_10_20/model\")\n",
    "\n",
    "# Initialize the environment and variables for recording\n",
    "frames = []\n",
    "obs, info = env_10_20.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "max_steps = 1000\n",
    "\n",
    "# Resize frame to be divisible by 16 (macro block size for video codecs)\n",
    "def resize_frame_to_macro_block_size(frame, block_size=16):\n",
    "    h, w, _ = frame.shape\n",
    "    new_w = (w // block_size) * block_size\n",
    "    new_h = (h // block_size) * block_size\n",
    "    return cv2.resize(frame, (new_w, new_h))\n",
    "\n",
    "# Run the agent in the environment\n",
    "while step_count < max_steps and not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env_10_20.step(action)\n",
    "    frame = env_10_20.render()\n",
    "\n",
    "    # Resize the frame to avoid the macro_block_size warning\n",
    "    resized_frame = resize_frame_to_macro_block_size(frame)\n",
    "    frames.append(resized_frame)\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "# Close the environment\n",
    "env_10_20.close()\n",
    "\n",
    "# Save the frames as a video\n",
    "video_filename = \"velocity_study.mp4\"\n",
    "imageio.mimsave(video_filename, frames, fps=30)\n",
    "print(f\"Video saved as {video_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the video\n",
    "video_filename = \"velocity_study.mp4\"\n",
    "Video(video_filename, embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bii)",
   "language": "python",
   "name": "bii"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
