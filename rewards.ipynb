{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pprint\n",
    "from highway_env.envs import MergeEnv\n",
    "from highway_env import utils\n",
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment is designed to have only two cars on the road: the ego vehicle on the highway and the merging vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards Break close to the merging car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personalized environment with custom reward function\n",
    "class CustomMergeEnv(MergeEnv):\n",
    "    def _make_vehicles(self) -> None:\n",
    "        \n",
    "        road = self.road\n",
    "        # Ego vehicle\n",
    "        ego_vehicle = self.action_type.vehicle_class(\n",
    "            road, road.network.get_lane((\"a\", \"b\", 1)).position(30, 0), speed=30\n",
    "        )\n",
    "        road.vehicles.append(ego_vehicle)\n",
    "\n",
    "        other_vehicles_type = utils.class_from_path(self.config[\"other_vehicles_type\"])\n",
    "\n",
    "        # Merging vehicle\n",
    "        merging_v = other_vehicles_type(\n",
    "            road, road.network.get_lane((\"j\", \"k\", 0)).position(110, 0), speed=20\n",
    "        )\n",
    "        merging_v.target_speed = 30\n",
    "        road.vehicles.append(merging_v)\n",
    "        \n",
    "        # Set the ego vehicle as the primary vehicle\n",
    "        self.vehicle = ego_vehicle\n",
    "\n",
    "    def _reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Custom reward function combining the original reward with proximity-based\n",
    "        braking behavior to allow the merging vehicle to merge safely.\n",
    "        \"\"\"\n",
    "        # Get the original reward from the parent class (if it exists)\n",
    "        reward = super()._reward(action)\n",
    "        \n",
    "        ego_vehicle = self.vehicle\n",
    "        road = self.road\n",
    "\n",
    "        # Find the merging vehicle (the vehicle in the merging lane)\n",
    "        merging_vehicle = None\n",
    "        for vehicle in road.vehicles:\n",
    "            if vehicle.lane_index == (\"j\", \"k\", 0):  # Assuming this is the merging lane\n",
    "                merging_vehicle = vehicle\n",
    "                break\n",
    "        \n",
    "        #print(\"ego\",ego_vehicle.position)\n",
    "        #print(\"merging\",vehicle.position)\n",
    "        \n",
    "        # Calculate distance to the merging vehicle\n",
    "        if not merging_vehicle:\n",
    "            return reward\n",
    "            \n",
    "            \n",
    "        distance=abs(ego_vehicle.position[0]-vehicle.position[0])    \n",
    "       \n",
    "        # Braking Incentive Near Merging Car\n",
    "        d_brake_threshold = 30  # Braking threshold distance (in meters)\n",
    "        gamma = 1.0  # Braking reward scaling factor\n",
    "\n",
    "        # Check if braking is needed (negative acceleration and speed condition)\n",
    "        if distance < d_brake_threshold and ego_vehicle.speed > merging_vehicle.speed:\n",
    "            acceleration = ego_vehicle.acceleration  # Negative for braking\n",
    "            if acceleration < 0:  # Ensure this is actual braking\n",
    "                braking_reward = gamma * abs(acceleration)\n",
    "            else:\n",
    "                braking_reward = 0.0\n",
    "        else:\n",
    "            braking_reward = 0.0\n",
    "        \n",
    "        # Distance-based Reward (Proximity to Merging Car)\n",
    "        beta = 0.1  # Controls how fast reward decays as distance increases\n",
    "        distance_reward = np.exp(-beta * (distance ** 2))\n",
    "\n",
    "        # Add both rewards to the original reward\n",
    "        reward += braking_reward + distance_reward\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomMergeEnv(render_mode='rgb_array',config={\"high_speed_reward\": 1, \"lane_change_reward\": -5, \"right_lane_reward\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env_brake_close/\")\n",
    "timesteps = 50000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env_brake_close/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards break far from the merging vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personalized environment with custom reward function\n",
    "class CustomMergeEnv(MergeEnv):\n",
    "    def _make_vehicles(self) -> None:\n",
    "        \n",
    "        road = self.road\n",
    "        # Ego vehicle\n",
    "        ego_vehicle = self.action_type.vehicle_class(\n",
    "            road, road.network.get_lane((\"a\", \"b\", 1)).position(30, 0), speed=30\n",
    "        )\n",
    "        road.vehicles.append(ego_vehicle)\n",
    "\n",
    "        other_vehicles_type = utils.class_from_path(self.config[\"other_vehicles_type\"])\n",
    "\n",
    "        # Merging vehicle\n",
    "        merging_v = other_vehicles_type(\n",
    "            road, road.network.get_lane((\"j\", \"k\", 0)).position(110, 0), speed=20\n",
    "        )\n",
    "        merging_v.target_speed = 30\n",
    "        road.vehicles.append(merging_v)\n",
    "        \n",
    "        # Set the ego vehicle as the primary vehicle\n",
    "        self.vehicle = ego_vehicle\n",
    "\n",
    "    def _reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Custom reward function combining the original reward with proximity-based\n",
    "        braking behavior and smooth braking to allow the merging vehicle to merge safely.\n",
    "        \"\"\"\n",
    "        # Get the original reward from the parent class (if it exists)\n",
    "        reward = super()._reward(action)\n",
    "        \n",
    "        ego_vehicle = self.vehicle\n",
    "        road = self.road\n",
    "\n",
    "        # Find the merging vehicle (the vehicle in the merging lane)\n",
    "        merging_vehicle = None\n",
    "        for vehicle in road.vehicles:\n",
    "            if vehicle.lane_index == (\"j\", \"k\", 0):  # Assuming this is the merging lane\n",
    "                merging_vehicle = vehicle\n",
    "                break\n",
    "\n",
    "        # Calculate distance to the merging vehicle\n",
    "        if not merging_vehicle:\n",
    "            return reward  # No merging vehicle, no proximity or braking rewards\n",
    "\n",
    "        distance = abs(ego_vehicle.position[0] - merging_vehicle.position[0])\n",
    "\n",
    "        ## Proximity Reward ##\n",
    "        d_min = 10  # Minimum safe distance\n",
    "        d_max = 30  # Maximum safe distance\n",
    "        \n",
    "        if d_min < distance < d_max:\n",
    "            proximity_reward = 1.0  # Reward for staying in a safe proximity to the merging vehicle\n",
    "        else:\n",
    "            proximity_reward = 0.0  # No reward if outside the safe distance range\n",
    "\n",
    "            ## Distance-based Penalty if too close to the merging vehicle ##\n",
    "        close_distance_threshold = 5  # Threshold for being \"too close\" to the merging vehicle\n",
    "\n",
    "        if distance < close_distance_threshold:\n",
    "            distance_penalty = -2.0  # Penalty for being too close to the merging vehicle\n",
    "        else:\n",
    "            distance_penalty = 0.0  # No penalty if not too close\n",
    "\n",
    "        ## Braking Incentive Near Merging Car ##\n",
    "        d_brake_threshold = 60  # Braking threshold distance (in meters)\n",
    "        gamma = 1.0  # Scaling factor for braking reward\n",
    "\n",
    "        # Reward braking if the ego vehicle is within threshold and going faster than merging vehicle\n",
    "        if distance < d_brake_threshold and ego_vehicle.speed > merging_vehicle.speed:\n",
    "            # Calculate deceleration (if ego_vehicle.previous_speed is available)\n",
    "            if not hasattr(ego_vehicle, 'previous_speed'):\n",
    "                ego_vehicle.previous_speed = ego_vehicle.speed  # Initialize previous speed\n",
    "\n",
    "            deceleration = ego_vehicle.previous_speed - ego_vehicle.speed  # Calculate deceleration\n",
    "            ego_vehicle.previous_speed = ego_vehicle.speed  # Update previous speed\n",
    "\n",
    "            if deceleration > 0:  # Check if decelerating\n",
    "                braking_incentive = gamma * deceleration  # Reward for braking\n",
    "            else:\n",
    "                braking_incentive = 0.0\n",
    "        else:\n",
    "            braking_incentive = 0.0\n",
    "\n",
    "        # Add proximity reward, distance penalty, and braking incentive to the original reward\n",
    "        reward += proximity_reward + braking_incentive + distance_penalty\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = CustomMergeEnv(render_mode='rgb_array',config={\"high_speed_reward\": 1, \"lane_change_reward\": -5, \"right_lane_reward\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env1,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            learning_rate=5e-4,\n",
    "            n_steps=2048, \n",
    "            batch_size=64, \n",
    "            n_epochs=10,  \n",
    "            gamma=0.8,\n",
    "            gae_lambda=0.95, \n",
    "            clip_range=0.2, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"env1_brake_close/\")\n",
    "timesteps = 50000\n",
    "model.learn(total_timesteps=timesteps)\n",
    "model.save(\"env1_brake_close/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(path):\n",
    "    # Create the environment\n",
    "    env = CustomMergeEnv(render_mode='rgb_array', config={\n",
    "        \"real_time_rendering\": True\n",
    "    })\n",
    "\n",
    "    # Load the trained model\n",
    "    model_path = os.path.join(path, \"model.zip\")  # Correctly join the path and model file name\n",
    "    model = PPO.load(model_path)  # Load the saved model\n",
    "\n",
    "    # Reset the environment and get the observation (ignore the info)\n",
    "    obs, _ = env.reset()  # Unpack obs and ignore info\n",
    "\n",
    "    # Run the simulation for a fixed number of steps or until the episode ends\n",
    "    for _ in range(100):  # Run for 100 steps\n",
    "        action, _states = model.predict(obs)  # Use the model to predict actions\n",
    "        obs, reward, done, truncated, info = env.step(action)  # Take the predicted action\n",
    "        env.render()  # Render the environment\n",
    "\n",
    "        if done or truncated:\n",
    "            obs, _ = env.reset()  # Reset the environment when an episode finishes and ignore info\n",
    "\n",
    "    # Display the final frame\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(\"env_brake_close\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
